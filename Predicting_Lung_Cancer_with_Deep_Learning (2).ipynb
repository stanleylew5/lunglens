{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x8eIyeOJfc5C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import os\n",
        "import glob as gb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary files"
      ],
      "metadata": {
        "id": "decCaGbsyJRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcuqK_Ibf2VA",
        "outputId": "5e8bdd7a-c480-43c6-ddac-e15b4b1cc4d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting data prep\n"
      ],
      "metadata": {
        "id": "47XnMRukNpKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data_dir = '/content/drive/MyDrive/rosehackProject/lungData/originalData'\n",
        "\n",
        "# train_dir = '/content/drive/MyDrive/rosehackProject/lungData/learningData/train'\n",
        "# test_dir = '/content/drive/MyDrive/rosehackProject/lungData/learningData/test'\n",
        "\n",
        "# os.makedirs(train_dir, exist_ok=True)\n",
        "# os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# test_size_ratio = 0.2\n",
        "\n",
        "# class_directories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "# class_directories = [d for d in class_directories if d not in ['train', 'test']]  # Exclude 'train' and 'test' if they are in the same parent directory\n",
        "\n",
        "# for class_dir in class_directories:\n",
        "#     class_dir_full = os.path.join(data_dir, class_dir)\n",
        "\n",
        "#     images = [img for img in os.listdir(class_dir_full) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "#     print(f'Class: {class_dir}, Total Images: {len(images)}')\n",
        "\n",
        "#     if images:\n",
        "#         train_images, test_images = train_test_split(images, test_size=test_size_ratio)\n",
        "#         os.makedirs(os.path.join(train_dir, class_dir), exist_ok=True)\n",
        "#         os.makedirs(os.path.join(test_dir, class_dir), exist_ok=True)\n",
        "\n",
        "#         for image in train_images:\n",
        "#             shutil.copy(os.path.join(class_dir_full, image), os.path.join(train_dir, class_dir))\n",
        "\n",
        "#         for image in test_images:\n",
        "#             shutil.copy(os.path.join(class_dir_full, image), os.path.join(test_dir, class_dir))\n",
        "#     else:\n",
        "#         print(f'No images found in directory: {class_dir_full}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUlC4HtCEZHf",
        "outputId": "cf09beee-e229-4615-f512-f39c06b0caff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: Malignant cases, Total Images: 561\n",
            "Class: Normal cases, Total Images: 416\n",
            "Class: Bengin cases, Total Images: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# # Paths to your data directories\n",
        "# normal_dir = 'data/Normal'\n",
        "# cancer_dir = 'data/Lung_Cancer'  # This should include both Benign and Malignant images\n",
        "\n",
        "# # Load images and labels\n",
        "# def load_images_and_labels(category_dir, label):\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     for filename in os.listdir(category_dir):\n",
        "#         if filename.endswith('.jpg') or filename.endswith('.png'):  # Check for image files\n",
        "#             img_path = os.path.join(category_dir, filename)\n",
        "#             img = load_img(img_path, target_size=(64, 64))  # Resize\n",
        "#             img = img_to_array(img)\n",
        "#             images.append(img)\n",
        "#             labels.append(label)\n",
        "#     return images, labels\n",
        "\n",
        "# # Load all images\n",
        "# normal_images, normal_labels = load_images_and_labels(normal_dir, 0)\n",
        "# cancer_images, cancer_labels = load_images_and_labels(cancer_dir, 1)\n",
        "\n",
        "# # Combine data\n",
        "# images = np.array(normal_images + cancer_images)\n",
        "# labels = np.array(normal_labels + cancer_labels)\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# train_x_orig, test_x_orig, train_y, test_y = train_test_split(images, labels, test_size=0.2, random_state=1)\n",
        "# train_y = train_y.reshape((1, train_y.shape[0]))\n",
        "# test_y = test_y.reshape((1, test_y.shape[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "ehCPQzK1OXZC",
        "outputId": "982be894-e004-49cd-92b6-cf76fe553213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/Normal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e17592323705>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnormal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mcancer_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancer_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancer_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-e17592323705>\u001b[0m in \u001b[0;36mload_images_and_labels\u001b[0;34m(category_dir, label)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check for image files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code segment we label our data.  For example, train_x_orig is for the actual image and train_x is the label.\n"
      ],
      "metadata": {
        "id": "qzlWL1AMOZIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# def reorganize_lung_cancer_data(base_dir):\n",
        "#     for category in ['train', 'test']:\n",
        "#         # Define paths for 'Normal', 'Benign', 'Malignant', and 'LungCancer' directories\n",
        "#         normal_dir = os.path.join(base_dir, category, 'Normal')\n",
        "#         benign_dir = os.path.join(base_dir, category, 'LungCancer', 'Benign')\n",
        "#         malignant_dir = os.path.join(base_dir, category, 'LungCancer', 'Malignant')\n",
        "#         lung_cancer_dir = os.path.join(base_dir, category, 'LungCancer')\n",
        "\n",
        "#         # Check if 'LungCancer' folder exists, if not, create it\n",
        "#         if not os.path.exists(lung_cancer_dir):\n",
        "#             os.makedirs(lung_cancer_dir)\n",
        "\n",
        "#         # Move all images from 'Benign' and 'Malignant' to 'LungCancer'\n",
        "#         for folder in [benign_dir, malignant_dir]:\n",
        "#             for filename in os.listdir(folder):\n",
        "#                 source = os.path.join(folder, filename)\n",
        "#                 destination = os.path.join(lung_cancer_dir, filename)\n",
        "#                 shutil.move(source, destination)\n",
        "\n",
        "#             # Remove the now empty 'Benign' and 'Malignant' folders\n",
        "#             os.rmdir(folder)\n",
        "\n",
        "# base_dir = '/content/drive/MyDrive/rosehackProject/lungData/learningData'  # Base directory containing 'train' and 'test' folders\n",
        "# reorganize_lung_cancer_data(base_dir)\n"
      ],
      "metadata": {
        "id": "D6gq7QMXJ9Ai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "31213630-153e-4931-a78c-a442debd93c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/rosehackProject/lungData/learningData/train/LungCancer/Benign'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e9aeea7877b0>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/rosehackProject/lungData/learningData'\u001b[0m  \u001b[0;31m# Base directory containing 'train' and 'test' folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mreorganize_lung_cancer_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-e9aeea7877b0>\u001b[0m in \u001b[0;36mreorganize_lung_cancer_data\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Move all images from 'Benign' and 'Malignant' to 'LungCancer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbenign_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmalignant_dir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlung_cancer_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/rosehackProject/lungData/learningData/train/LungCancer/Benign'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start here\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# Paths to your data directories\n",
        "normal_dir = '/content/drive/MyDrive/rosehackProject/lungData/learningData/train/Normal cases'\n",
        "cancer_dir = '/content/drive/MyDrive/rosehackProject/lungData/learningData/train/LungCancer'  # This should include both Benign and Malignant images\n",
        "\n",
        "# Load images and labels\n",
        "def load_images_and_labels(category_dir, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(category_dir):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Check for image files\n",
        "            img_path = os.path.join(category_dir, filename)\n",
        "            img = load_img(img_path, target_size=(64, 64))  # Resize\n",
        "            img = img_to_array(img)\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Load all images\n",
        "normal_images, normal_labels = load_images_and_labels(normal_dir, 0)\n",
        "cancer_images, cancer_labels = load_images_and_labels(cancer_dir, 1)\n",
        "\n",
        "# Combine data\n",
        "images = np.array(normal_images + cancer_images)\n",
        "labels = np.array(normal_labels + cancer_labels)\n",
        "\n",
        "# Shuffle the data\n",
        "permutation = np.random.permutation(images.shape[0])\n",
        "images = images[permutation]\n",
        "labels = labels[permutation]\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_x_orig, test_x_orig, train_y, test_y = train_test_split(images, labels, test_size=0.2, random_state=1)\n",
        "train_y = train_y.reshape((1, train_y.shape[0]))\n",
        "test_y = test_y.reshape((1, test_y.shape[0]))\n"
      ],
      "metadata": {
        "id": "Bbc8f5cVR6mu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the images\n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Normalize pixel values\n",
        "train_x = train_x_flatten / 255.\n",
        "test_x = test_x_flatten / 255.\n"
      ],
      "metadata": {
        "id": "G7nLPq8cSNMj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "\n",
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
        "print (\"train_y shape: \" + str(train_y.shape))\n",
        "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
        "print (\"test_y shape: \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLiB7MAtSSvI",
        "outputId": "ecbceabd-f67a-47b5-c58f-0c987dedcda2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 856\n",
            "Number of testing examples: 214\n",
            "Each image is of size: (64, 64, 3)\n",
            "train_x_orig shape: (856, 64, 64, 3)\n",
            "train_y shape: (1, 856)\n",
            "test_x_orig shape: (214, 64, 64, 3)\n",
            "test_y shape: (1, 214)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_x = train_x.shape[0]\n",
        "n_h = 856\n",
        "n_y = 1\n",
        "layers_dims = (n_x, n_h, n_y)"
      ],
      "metadata": {
        "id": "nfZRI1G0TGhi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
        "\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "EFBILN5oTwDK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "HmCAh9lsUPp7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    Z = np.dot(W, A) + b\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "CiM77zH3VvF2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    # Shorten the code\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "Ht8LOC-aV9Zj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    A = np.maximum(0,Z)\n",
        "\n",
        "    assert(A.shape == Z.shape)\n",
        "\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "\n",
        "    # When z <= 0, you should set dz to 0 as well.\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "sjYMBkrqXhf2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "4aZwsF35Xx5a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    dW = np.dot(dZ, cache[0].T) / m\n",
        "    db = np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
        "    dA_prev = np.dot(cache[1].T, dZ)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (isinstance(db, float) or isinstance(db, np.ndarray))\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "uFmpHBIXZqPk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "3wnb2bFQbQ9C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []                              # to keep track of the cost\n",
        "    m = X.shape[1]                           # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "\n",
        "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
        "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute cost\n",
        "        cost = compute_cost(A2, Y)\n",
        "\n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "\n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
        "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "\n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (approx. 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "\n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "U0e9HqMQTJO4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdmX-MYKTRz1",
        "outputId": "c9e7ec67-ee2c-4bcd-b3fd-1805bf9f702e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.6801507821173537\n",
            "Cost after iteration 100: 0.5039351384886523\n",
            "Cost after iteration 200: 0.40444764272131867\n",
            "Cost after iteration 300: 0.3169024736602086\n",
            "Cost after iteration 400: 0.2414812371335257\n",
            "Cost after iteration 500: 0.18233610120856428\n",
            "Cost after iteration 600: 0.13876405795058075\n",
            "Cost after iteration 700: 0.10707710064441907\n",
            "Cost after iteration 800: 0.08456967937946676\n",
            "Cost after iteration 900: 0.06825682288608592\n",
            "Cost after iteration 1000: 0.05618395900015485\n",
            "Cost after iteration 1100: 0.04705410063788765\n",
            "Cost after iteration 1200: 0.04001290302278969\n",
            "Cost after iteration 1300: 0.0344862525348571\n",
            "Cost after iteration 1400: 0.030076361723621343\n",
            "Cost after iteration 1500: 0.026505682766819694\n",
            "Cost after iteration 1600: 0.023573613335889025\n",
            "Cost after iteration 1700: 0.021137580421602107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the parameters to a file\n",
        "with open('parameters.pkl', 'wb') as file:\n",
        "    pickle.dump(parameters, file)"
      ],
      "metadata": {
        "id": "RCVe7okJq7JX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}